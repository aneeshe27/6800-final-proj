# -*- coding: utf-8 -*-
"""CIS 6800 Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbXQkK7936rY14ty-ngZqTZNux8uA5QE

# Data & Dependency Loading
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 -q
!pip install torch-geometric -q
!pip install opencv-python imageio tqdm matplotlib -q
# Install AWS CLI (use version 2)
!pip install awscli --upgrade

# Now configure ‚Äî this will prompt for Access Key, Secret, region, output format
!aws configure

import os
import cv2
import json
import torch
import numpy as np
import imageio
from glob import glob
from tqdm import tqdm
from torch_geometric.data import Data, Dataset, DataLoader

# Cell 0: Install dependencies (Ego4D CLI, plus ffmpeg if you want to extract frames later)
!pip install ego4d --upgrade -q
!apt-get update -qq && apt-get install -y ffmpeg -qq

# Cell 2: Set up download parameters
# Change OUTPUT_DIR to a Drive folder or local folder as you prefer
OUTPUT_DIR="/content/drive/MyDrive/CIS 6800 Final Project/ego4d_data"
BENCHMARK="FHO"   # Hands & Objects / PNR-relevant

# Cell 3: Download annotations + benchmark clips for Hands & Objects (FHO)
!ego4d --output_directory="$OUTPUT_DIR" --datasets annotations --benchmarks $BENCHMARK --metadata -y

# =====================================================================
# 0. SETUP
# =====================================================================
import json, random, pathlib, os

ROOT = pathlib.Path("/content/drive/MyDrive/CIS 6800 Final Project/ego4d_data/v2")
ANN_DIR = ROOT / "annotations"
SPLIT_DIR = ROOT / "splits"
SPLIT_DIR.mkdir(parents=True, exist_ok=True)

print("ROOT =", ROOT)

# =====================================================================
# 1. Collect PNR video_uids DIRECTLY FROM ANNOTATIONS
# =====================================================================
annotation_files = [
    "fho_oscc-pnr_train.json",
    "fho_oscc-pnr_val.json",
]

def has_pnr(c):
    return ("clip_pnr_frame" in c) or any("pnr_frame" in f for f in c.get("frames", []))

video_uids = set()

for name in annotation_files:
    path = ANN_DIR / name
    print("Loading:", path)
    data = json.load(open(path))

    for c in data["clips"]:
        if has_pnr(c) and c.get("video_uid"):
            video_uids.add(c["video_uid"])

video_uids = list(video_uids)
print("Found", len(video_uids), "PNR video_uids from annotations")


# =====================================================================
# 2. Sample EXACTLY 36 Videos
# =====================================================================
random.seed(0)
random.shuffle(video_uids)

if len(video_uids) < 36:
    raise RuntimeError("Not enough PNR videos in annotations.")

sampled = video_uids[:36]

train = sampled[:28]
val   = sampled[28:32]
test  = sampled[32:36]

print("\n=== FINAL SPLIT ===")
print("Train:", len(train))
print("Val:", len(val))
print("Test:", len(test))


# =====================================================================
# 3. Save lists (These are passed directly to ego4d CLI)
# =====================================================================
DL_DIR = ROOT / "download_lists"
DL_DIR.mkdir(exist_ok=True)

TRAIN_FILE = DL_DIR / "train.txt"
VAL_FILE   = DL_DIR / "val.txt"
TEST_FILE  = DL_DIR / "test.txt"

open(TRAIN_FILE, "w").write("\n".join(train))
open(VAL_FILE, "w").write("\n".join(val))
open(TEST_FILE, "w").write("\n".join(test))

print("\nSaved lists to:", DL_DIR)


# =====================================================================
# 4. Create output directories
# =====================================================================
TRAIN_OUT = ROOT / "train_videos"
VAL_OUT   = ROOT / "val_videos"
TEST_OUT  = ROOT / "test_videos"

TRAIN_OUT.mkdir(exist_ok=True)
VAL_OUT.mkdir(exist_ok=True)
TEST_OUT.mkdir(exist_ok=True)


# =====================================================================
# 5. Download ONLY these videos using video_540ss_clip
# =====================================================================
print("\n=== DOWNLOADING TRAIN VIDEOS (28) ===")
!ego4d -o "$TRAIN_OUT" \
    --datasets video_540ss \
    --video_uid_file "$TRAIN_FILE" \
    --metadata -y

print("\n=== DOWNLOADING VAL VIDEOS (4) ===")
!ego4d -o "$VAL_OUT" \
    --datasets video_540ss \
    --video_uid_file "$VAL_FILE" \
    --metadata -y

print("\n=== DOWNLOADING TEST VIDEOS (4) ===")
!ego4d -o "$TEST_OUT" \
    --datasets video_540ss \
    --video_uid_file "$TEST_FILE" \
    --metadata -y

print("\nüéâ DONE ‚Äî ONLY YOUR 36 VIDEOS WERE DOWNLOADED.")

"""# Graph Induction From Frames"""

# Commented out IPython magic to ensure Python compatibility.
# ===============================
# Bootstrap + Download + Load Cell (WITH SAM 2 - SIMPLEST)
# ===============================

import os
from pathlib import Path

PROJECT_ROOT = Path("/content/drive/MyDrive/CIS 6800 Final Project")
REPO_DIR = PROJECT_ROOT / "Grounded-Segment-Anything"
CKPT_DIR = PROJECT_ROOT / "checkpoints"

os.makedirs(REPO_DIR, exist_ok=True)
os.makedirs(CKPT_DIR, exist_ok=True)

# 1) Clone repos
if not (REPO_DIR / "GroundingDINO").exists():
#     %cd "$PROJECT_ROOT"
    !git clone https://github.com/IDEA-Research/Grounded-Segment-Anything "$REPO_DIR"

# 2) Download pretrained checkpoints
# %cd "$CKPT_DIR"

# GroundingDINO checkpoint
!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth \
      -O groundingdino_swint_ogc.pth

# SAM 2 checkpoint (using SAM 2.1 Large model)
!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt \
      -O sam2.1_hiera_large.pt

# 3) Install dependencies
!pip install -q torch torchvision torchaudio
!pip install -q opencv-python pillow matplotlib
!pip install -q groundingdino-py
!pip install -q 'git+https://github.com/facebookresearch/segment-anything-2.git'

# 4) Verify files exist
CFG = REPO_DIR / "GroundingDINO" / "groundingdino" / "config" / "GroundingDINO_SwinT_OGC.py"
CKPT = CKPT_DIR / "groundingdino_swint_ogc.pth"
SAM2_CKPT = CKPT_DIR / "sam2.1_hiera_large.pt"

print("GroundingDINO CFG exists:", CFG.exists())
print("GroundingDINO checkpoint exists:", CKPT.exists())
print("SAM 2 checkpoint exists:", SAM2_CKPT.exists())

assert CFG.exists(), f"CONFIG NOT FOUND: {CFG}"
assert CKPT.exists(), f"GROUNDING DINO CHECKPOINT NOT FOUND: {CKPT}"
assert SAM2_CKPT.exists(), f"SAM 2 CHECKPOINT NOT FOUND: {SAM2_CKPT}"

# 5) Load GroundingDINO + SAM 2
from groundingdino.util.inference import load_model
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load GroundingDINO
model = load_model(str(CFG), str(CKPT), device=device)

# Load SAM 2 using direct import
print("Loading SAM 2...")
from sam2.sam2_image_predictor import SAM2ImagePredictor

# Initialize SAM 2 with automatic model detection from checkpoint
sam_predictor = SAM2ImagePredictor.from_pretrained(
    "facebook/sam2.1-hiera-large",
    checkpoint=str(SAM2_CKPT)
)
sam_predictor.model.to(device)

print("‚úÖ GroundingDINO + SAM 2 loaded successfully on", device)

# ===============================
# Two-Stage Detection with Comprehensive Ego4D Ontology Prompts
# ===============================

import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from groundingdino.util.inference import load_image, predict
import torch
import networkx as nx

# Configuration
BOX_THRESHOLD = 0.3
TEXT_THRESHOLD = 0.25
IOU_THRESHOLD = 0.01
NMS_THRESHOLD = 0.5

# ===============================
# COMPREHENSIVE PROMPTS FROM EGO4D ONTOLOGY
# ===============================

# Hands and body parts (for interaction detection)
HAND_PROMPT = "hand . left hand . right hand . finger . palm . thumb"

# Objects grouped by semantic category (8-10 items per prompt for optimal detection)
OBJECT_PROMPTS = [
    # Kitchen Utensils - Cutting & Prep
    "knife . spoon . fork . spatula . ladle . whisk . peeler . grater . tongs . chopstick",
    "scissor . shears . can opener . cutter . scraper . slicer",

    # Kitchen Utensils - Cooking Tools
    "scoop . sieve . strainer . colander . funnel . skewer . toothpick . roller",

    # Containers - Cookware
    "pot . pan . frypan . saucepan . bowl . plate . dish . platter . saucer",
    "cup . mug . tumbler . glass . bottle . flask . jar . jug . beaker",
    "container . box . can . carton . crate . packet . tin . tank",
    "bucket . basket . tray . lid . cap . cover",

    # Food - Vegetables
    "onion . tomato . carrot . potato . garlic . pepper . capsicum . cucumber",
    "cabbage . lettuce . broccoli . spinach . celery . asparagus . kale",
    "mushroom . corn . pea . bean . zucchini . courgette . eggplant . squash",
    "radish . turnip . leek . okra . ginger . yam . cassava",

    # Food - Fruits
    "apple . banana . orange . lemon . lime . mango . pineapple . watermelon",
    "strawberry . grape . cherry . peach . pear . plum . kiwi . melon",
    "avocado . coconut . papaya . guava . grapefruit . berry",

    # Food - Proteins & Dairy
    "meat . beef . ham . chicken . lamb . bacon . sausage . fish . shrimp",
    "egg . milk . butter . cheese . cream . yoghurt",

    # Food - Prepared & Staples
    "bread . bun . loaf . toast . tortilla . chapati . roti . flatbread",
    "rice . pasta . noodle . spaghetti . dough . flour . cereal",
    "pizza . burger . sandwich . hotdog . taco . burrito . salad . soup",
    "cake . cookie . pie . muffin . cupcake . brownie . doughnut . pastry . pancake",

    # Food - Condiments & Ingredients
    "sauce . ketchup . vinegar . oil . spice . seasoning . salt . sugar",
    "water . tea . coffee . beverage . juice . chocolate . cocoa . honey",

    # Hand Tools - General
    "hammer . mallet . screwdriver . wrench . spanner . pliers . chisel",
    "axe . saw . hacksaw . chainsaw . file . crowbar . pry bar",

    # Hand Tools - Specialized
    "drill . driller . drill bit . clamp . vice . jack . lever",
    "tape measure . ruler . spirit level . caliper . gauge . set square",
    "sandpaper . sander . planer . grinder . sharpener",

    # Hand Tools - Fastening
    "screw . bolt . nail . nut . washer . clamp . clip . pin",
    "screwdriver . wrench . ratchet . socket . allen key",

    # Garden & Outdoor Tools
    "shovel . spade . hoe . rake . trowel . sickle . shears . trimmer . pruner",
    "broom . broomstick . mop . dustpan . duster",
    "wheelbarrow . hose . nozzle . spray . sprayer . mower . lawnmower . blower",

    # Cleaning Supplies
    "sponge . scrubber . brush . cloth . rag . towel . napkin . tissue . wipe",
    "soap . detergent . cleaner . polish",

    # Adhesives & Fasteners
    "tape . sellotape . duct tape . glue . adhesive . sealant . glue gun",
    "rope . string . thread . twine . yarn . wire . cable . cord . rubber band",

    # Electronics & Devices
    "phone . cellphone . smartphone . tablet . computer . laptop . ipad",
    "remote . remote control . keyboard . mouse . camera . calculator",
    "television . tv . screen . monitor . speaker . microphone . headphone",

    # Office & Stationery
    "pen . marker . pencil . crayon . eraser . rubber . sharpener",
    "paper . notebook . book . magazine . card . envelope . stamp . sticker",
    "stapler . clip . pin . rubber band . tape . scissor",

    # Materials - Wood & Metal
    "wood . plank . board . plywood . timber . lumber . log . stick . twig",
    "metal . steel . rod . pipe . wire . nail . screw . bolt . chain",

    # Materials - Other
    "cardboard . paperboard . foam . plastic . brick . stone . rock . tile",
    "cement . concrete . mortar . putty . clay . sand . soil . dirt",

    # Household Items
    "bag . sack . pouch . purse . wallet . suitcase . backpack",
    "blanket . bedsheet . duvet . pillow . cushion . mat . rug . carpet",
    "towel . cloth . napkin . handkerchief . apron . glove",

    # Furniture & Fixtures
    "table . stand . chair . stool . bench . shelf . rack . cabinet . cupboard",
    "drawer . door . window . handle . knob . switch . button . lock . key",

    # Appliances
    "stove . burner . oven . microwave . fridge . refrigerator . freezer",
    "blender . mixer . juicer . toaster . kettle . cooker . steamer",
    "dishwasher . washing machine . vacuum . vacuum cleaner . iron",

    # Plumbing & Fixtures
    "sink . basin . faucet . tap . bathtub . shower . toilet . drain . pipe . valve",

    # Lighting
    "light . lamp . bulb . flashlight . torch . candle",

    # Personal Items
    "glasses . spectacle . goggle . sunglass . watch . ring . bracelet . necklace",
    "comb . brush . toothbrush . razor . razor blade . mirror",
    "shoe . boot . sandal . slipper . sock . hat . helmet . mask . facemask",

    # Craft & Art Supplies
    "paintbrush . paint . palette . canvas . ink . crayon . chalk",
    "needle . thread . yarn . wool . fabric . cloth . button . zipper",

    # Medical & Safety
    "bandage . gauze . medicine . syringe . thermometer . inhaler",
    "glove . mask . facemask . goggle . helmet",

    # Miscellaneous Tools
    "funnel . sieve . strainer . pump . nozzle . valve . gauge",
    "magnet . spring . gear . bearing . gasket . washer",
]

# ===============================
# Helper Functions
# ===============================

def process_sam2_mask(mask):
    """Convert SAM2 mask output to 2D boolean array."""
    if isinstance(mask, torch.Tensor):
        mask = mask.cpu().numpy()
    while mask.ndim > 2:
        mask = mask.squeeze(0)
    if mask.dtype in [np.float32, np.float64, np.float16]:
        mask = mask > 0.5
    return mask.astype(bool)

def box_iou(box1, box2):
    """Compute IoU between two boxes in cxcywh format."""
    b1_x1 = box1[0] - box1[2]/2
    b1_y1 = box1[1] - box1[3]/2
    b1_x2 = box1[0] + box1[2]/2
    b1_y2 = box1[1] + box1[3]/2

    b2_x1 = box2[0] - box2[2]/2
    b2_y1 = box2[1] - box2[3]/2
    b2_x2 = box2[0] + box2[2]/2
    b2_y2 = box2[1] + box2[3]/2

    inter_x1 = max(b1_x1, b2_x1)
    inter_y1 = max(b1_y1, b2_y1)
    inter_x2 = min(b1_x2, b2_x2)
    inter_y2 = min(b1_y2, b2_y2)

    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)

    return inter_area / (b1_area + b2_area - inter_area + 1e-6)

def nms_detections(boxes, logits, phrases, is_hand, iou_thresh=0.5):
    """Remove duplicate detections using NMS."""
    if len(boxes) == 0:
        return [], [], [], []

    # Sort by confidence (descending)
    indices = sorted(range(len(logits)), key=lambda i: logits[i].item(), reverse=True)

    keep = []
    for i in indices:
        should_keep = True
        for j in keep:
            if box_iou(boxes[i], boxes[j]) > iou_thresh:
                should_keep = False
                break
        if should_keep:
            keep.append(i)

    return (
        [boxes[i] for i in keep],
        [logits[i] for i in keep],
        [phrases[i] for i in keep],
        [is_hand[i] for i in keep]
    )

def compute_mask_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:
    """Compute IoU between two binary masks."""
    intersection = np.logical_and(mask1, mask2).sum()
    union = np.logical_or(mask1, mask2).sum()
    if union == 0:
        return 0.0
    return float(intersection) / float(union)

# ===============================
# Index downloaded videos + collect PNR clips
# ===============================
import json
from pathlib import Path

ROOT = Path("/content/drive/MyDrive/CIS 6800 Final Project/ego4d_data/v2")

VIDEO_SPLITS = {
    "train": ROOT / "train_videos",
    "val":   ROOT / "val_videos",
    "test":  ROOT / "test_videos",
}

ANN_ROOT = ROOT / "annotations"
PNR_ANN_FILES = [
    ANN_ROOT / "fho_oscc-pnr_train.json",
    ANN_ROOT / "fho_oscc-pnr_val.json",
]

# -------------------------------
# 1. Build video_uid -> {split, path}
# -------------------------------
video_index = {}

for split_name, split_dir in VIDEO_SPLITS.items():
    if not split_dir.exists():
        print(f"[WARN] Split dir not found: {split_dir}")
        continue

    for mp4 in split_dir.rglob("*.mp4"):
        vid_uid = mp4.stem  # assumes {video_uid}.mp4
        video_index[vid_uid] = {
            "split": split_name,
            "path": mp4,
        }

print(f"Indexed {len(video_index)} videos from train/val/test.")

# -------------------------------
# 2. Collect PNR clips that have videos
# -------------------------------
def normalize_ann_data(data):
    """Handle both {'clips': [...]} and list-of-dicts formats."""
    if isinstance(data, dict) and "clips" in data:
        return data["clips"]
    if isinstance(data, list):
        return data
    raise ValueError("Unknown annotation JSON format")

def get_pnr_frame_from_clip(c):
    """
    Try several patterns:
    - 'clip_pnr_frame' at clip level
    - a frame dict in 'frames' that has pnr info
    """
    if "clip_pnr_frame" in c:
        return c["clip_pnr_frame"]

    frames = c.get("frames", [])
    for f in frames:
        # Try multiple key patterns
        if f.get("is_pnr", False):
            return f.get("frame_number") or f.get("frame_index") or f.get("pnr_frame")
        if "pnr_frame" in f:
            return f["pnr_frame"]

    return None

pnr_clips = []

for ann_path in PNR_ANN_FILES:
    print(f"Loading PNR annotations from {ann_path}")
    data = json.load(open(ann_path, "r"))
    clips = normalize_ann_data(data)

    for c in clips:
        vid = c.get("video_uid")
        if not vid or vid not in video_index:
            continue

        pnr_frame = get_pnr_frame_from_clip(c)
        if pnr_frame is None:
            continue

        pnr_clips.append({
            "video_uid": vid,
            "clip_uid": c.get("clip_uid"),
            "pnr_frame": int(pnr_frame),
            "split": video_index[vid]["split"],
        })

print(f"\nPNR clips with video on disk: {len(pnr_clips)}")
print("Sample:", pnr_clips[:5])

# ============================================================
# Install PyTorch Geometric for Graph Neural Networks
# ============================================================
!pip install -q torch_geometric
!pip install -q pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \
    -f https://data.pyg.org/whl/torch-2.2.0+cu121.html

import torch
print("PyTorch version:", torch.__version__)
try:
    from torch_geometric.data import Data
    print("‚úÖ PyTorch Geometric installed successfully!")
except Exception as e:
    print("‚ùå ERROR loading PyG:", e)

# ============================================================
# CELL 2: Extract frames with RANDOMIZED PNR position
# ============================================================
import cv2
import os
import random
from pathlib import Path
from tqdm import tqdm
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt

random.seed(42)  # For reproducibility

ROOT = Path("/content/drive/MyDrive/CIS 6800 Final Project/ego4d_data/v2")
FRAMES_ROOT = ROOT / "pnr_frames_v2"  # NEW folder - don't overwrite old data
FRAMES_ROOT.mkdir(parents=True, exist_ok=True)

FRAME_SKIP = 2
MIN_WINDOW = 3    # Minimum frames on each side of PNR
MAX_WINDOW = 10   # Maximum frames on each side of PNR

# ============================================================
# BALANCED SAMPLING CONFIGURATION
# ============================================================
N_TRAIN = 240
N_VAL = 30
N_TEST = 30

# ============================================================
# Separate clips by split
# ============================================================
train_clips = [c for c in pnr_clips if c["split"] == "train"]
val_clips = [c for c in pnr_clips if c["split"] == "val"]
test_clips = [c for c in pnr_clips if c["split"] == "test"]

print("=" * 50)
print("AVAILABLE CLIPS")
print("=" * 50)
print(f"  Train: {len(train_clips)}")
print(f"  Val:   {len(val_clips)}")
print(f"  Test:  {len(test_clips)}")
print(f"  Total: {len(pnr_clips)}")

# ============================================================
# Take requested amounts (with safety checks)
# ============================================================
n_train = min(N_TRAIN, len(train_clips))
n_val = min(N_VAL, len(val_clips))
n_test = min(N_TEST, len(test_clips))

if n_train < N_TRAIN:
    print(f"‚ö†Ô∏è Warning: Only {len(train_clips)} train clips available, requested {N_TRAIN}")
if n_val < N_VAL:
    print(f"‚ö†Ô∏è Warning: Only {len(val_clips)} val clips available, requested {N_VAL}")
if n_test < N_TEST:
    print(f"‚ö†Ô∏è Warning: Only {len(test_clips)} test clips available, requested {N_TEST}")

clips_to_process = (
    train_clips[:n_train] +
    val_clips[:n_val] +
    test_clips[:n_test]
)

print("\n" + "=" * 50)
print("CLIPS TO PROCESS")
print("=" * 50)
print(f"  Train: {n_train}")
print(f"  Val:   {n_val}")
print(f"  Test:  {n_test}")
print(f"  Total: {len(clips_to_process)}")

# ============================================================
# Frame extraction function - RANDOMIZED VERSION
# ============================================================
def extract_pnr_frames_randomized(video_path, pnr_frame, clip_id, output_dir, skip=2):
    """
    Extract frames with RANDOMIZED asymmetric window.
    PNR will appear at different positions in different clips.
    """
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print(f"[ERROR] Cannot open video: {video_path}")
        return []

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # RANDOMIZED: Different number of frames before vs after PNR
    frames_before = random.randint(MIN_WINDOW, MAX_WINDOW)
    frames_after = random.randint(MIN_WINDOW, MAX_WINDOW)

    start_frame = max(0, pnr_frame - frames_before * skip)
    end_frame = min(total_frames - 1, pnr_frame + frames_after * skip)

    clip_dir = output_dir / clip_id
    clip_dir.mkdir(parents=True, exist_ok=True)

    extracted_paths = []
    frame_idx = start_frame

    while frame_idx <= end_frame:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if ret:
            is_pnr = (frame_idx == pnr_frame)
            suffix = "_PNR" if is_pnr else ""
            frame_path = clip_dir / f"frame_{frame_idx:06d}{suffix}.jpg"
            cv2.imwrite(str(frame_path), frame)
            extracted_paths.append({
                "path": frame_path,
                "frame_idx": frame_idx,
                "is_pnr": is_pnr,
                "relative_to_pnr": frame_idx - pnr_frame
            })
        frame_idx += skip

    cap.release()
    return extracted_paths

# ============================================================
# Extract frames
# ============================================================
extracted_clips = []
pnr_positions = []  # Track where PNR ends up in each clip

for clip_info in tqdm(clips_to_process, desc="Extracting frames"):
    vid_uid = clip_info["video_uid"]
    pnr_frame = clip_info["pnr_frame"]
    split = clip_info["split"]

    video_path = video_index[vid_uid]["path"]
    clip_id = f"{vid_uid}_{pnr_frame}"

    frames = extract_pnr_frames_randomized(
        video_path, pnr_frame, clip_id,
        FRAMES_ROOT / split,
        skip=FRAME_SKIP
    )

    if frames:
        # Find where PNR ended up in this clip
        pnr_idx = next(i for i, f in enumerate(frames) if f["is_pnr"])
        seq_len = len(frames)
        relative_pos = pnr_idx / (seq_len - 1) if seq_len > 1 else 0.5
        pnr_positions.append(relative_pos)

        extracted_clips.append({
            "clip_id": clip_id,
            "video_uid": vid_uid,
            "pnr_frame": pnr_frame,
            "split": split,
            "frames": frames
        })

# ============================================================
# Verify PNR position distribution (IMPORTANT!)
# ============================================================
plt.figure(figsize=(10, 4))
plt.hist(pnr_positions, bins=20, edgecolor='black', alpha=0.7)
plt.xlabel('Relative PNR Position (0=start, 1=end)')
plt.ylabel('Count')
plt.title('PNR Position Distribution (Should be spread out, NOT all at 0.5)')
plt.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Middle')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"\nüìä PNR Position Statistics:")
print(f"   Mean: {np.mean(pnr_positions):.3f} (ideal: ~0.5)")
print(f"   Std:  {np.std(pnr_positions):.3f} (should be > 0.1, yours was 0.0)")
print(f"   Min:  {np.min(pnr_positions):.3f}")
print(f"   Max:  {np.max(pnr_positions):.3f}")

# ============================================================
# Verify final distribution
# ============================================================
final_counts = Counter(c["split"] for c in extracted_clips)
seq_lens = [len(c["frames"]) for c in extracted_clips]

print("\n" + "=" * 50)
print("‚úÖ EXTRACTION COMPLETE")
print("=" * 50)
print(f"  Train: {final_counts.get('train', 0)} clips")
print(f"  Val:   {final_counts.get('val', 0)} clips")
print(f"  Test:  {final_counts.get('test', 0)} clips")
print(f"  Total: {len(extracted_clips)} clips")
print(f"  Frames per clip: min={min(seq_lens)}, max={max(seq_lens)}, avg={np.mean(seq_lens):.1f}")
print(f"  Total frames: {sum(seq_lens)}")

# ============================================================
# Check baseline accuracy with new distribution
# ============================================================
print("\n" + "=" * 50)
print("BASELINE CHECK: Always Predict Middle")
print("=" * 50)

correct = 0
for clip, pos in zip(extracted_clips, pnr_positions):
    seq_len = len(clip["frames"])
    pred_middle = seq_len // 2
    pnr_idx = next(i for i, f in enumerate(clip["frames"]) if f["is_pnr"])
    if pred_middle == pnr_idx:
        correct += 1

baseline_acc = correct / len(extracted_clips)
print(f"  Baseline accuracy: {baseline_acc:.3f} ({correct}/{len(extracted_clips)})")
print(f"  (Was 1.0 before, should now be ~0.1-0.3)")

if baseline_acc > 0.5:
    print("  ‚ö†Ô∏è WARNING: Baseline still too high! PNR positions not random enough.")
else:
    print("  ‚úÖ Good! Task is now non-trivial.")

# ============================================================
# CELL 3: Core Graph Extraction Function (FIXED - handles occlusion)
# ============================================================
import torch
import numpy as np
from torch_geometric.data import Data
from groundingdino.util.inference import load_image, predict
from scipy.ndimage import binary_dilation
import networkx as nx

# Configuration
BOX_THRESHOLD = 0.3
TEXT_THRESHOLD = 0.25
NMS_THRESHOLD = 0.5

# Interaction detection thresholds
MASK_IOU_THRESHOLD = 0.01
BOX_IOU_THRESHOLD = 0.1
ADJACENCY_THRESHOLD = 0.1
DILATION_PIXELS = 15

# Excluded background objects (not manipulable)
EXCLUDED_LABELS = [
    "person", "table", "chair", "door", "shelf", "cabinet",
    "stove", "oven", "sink", "fridge", "wall", "floor",
    "ceiling", "window", "countertop", "counter", "background",
    "stool", "bench"
]

def compute_box_iou(box1, box2):
    """Compute IoU between two boxes in xyxy format."""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

    return inter / (area1 + area2 - inter + 1e-6)

def compute_interaction_score(hand_det, obj_det):
    """
    Compute interaction score using multiple signals to handle occlusion.
    """
    # Method 1: Direct mask IoU
    mask_iou = compute_mask_iou(hand_det["mask"], obj_det["mask"])

    # Method 2: Bounding box IoU (catches occlusion!)
    box_iou = compute_box_iou(hand_det["bbox"], obj_det["bbox"])

    # Method 3: Dilated mask adjacency
    dilated_hand = binary_dilation(hand_det["mask"], iterations=DILATION_PIXELS)
    dilated_overlap = np.logical_and(dilated_hand, obj_det["mask"]).sum()
    adjacency = dilated_overlap / (obj_det["mask"].sum() + 1e-6)

    # Combined score
    score = max(mask_iou, box_iou * 0.7, adjacency * 0.8)

    details = {
        "mask_iou": mask_iou,
        "box_iou": box_iou,
        "adjacency": adjacency
    }

    return score, details

def check_interaction(hand_det, obj_det):
    """Check if hand and object are interacting using multiple methods."""
    score, details = compute_interaction_score(hand_det, obj_det)

    is_interacting = (
        details["mask_iou"] > MASK_IOU_THRESHOLD or
        details["box_iou"] > BOX_IOU_THRESHOLD or
        details["adjacency"] > ADJACENCY_THRESHOLD
    )

    return is_interacting, score, details

def process_frame_to_graph(image_path, return_detections=False, verbose=False):
    """
    Process a single frame and return a PyTorch Geometric Data object.

    FIXED: Handles occlusion cases using box IoU and mask adjacency.
    """
    try:
        image_source, image = load_image(str(image_path))
        H, W, _ = image_source.shape

        def norm(val, max_val):
            return float(val) / float(max_val + 1e-8)

        # Stage 1: Detect hands
        all_boxes, all_logits, all_phrases, all_is_hand = [], [], [], []

        boxes_h, logits_h, phrases_h = predict(
            model=model, image=image, caption=HAND_PROMPT,
            box_threshold=BOX_THRESHOLD, text_threshold=TEXT_THRESHOLD, device=device
        )

        for i in range(len(boxes_h)):
            all_boxes.append(boxes_h[i])
            all_logits.append(logits_h[i])
            all_phrases.append(phrases_h[i])
            all_is_hand.append(True)

        if verbose:
            print(f"  Hands detected: {len(boxes_h)}")

        # Stage 2: Detect objects
        for prompt in OBJECT_PROMPTS:
            boxes_o, logits_o, phrases_o = predict(
                model=model, image=image, caption=prompt,
                box_threshold=BOX_THRESHOLD, text_threshold=TEXT_THRESHOLD, device=device
            )
            for i in range(len(boxes_o)):
                all_boxes.append(boxes_o[i])
                all_logits.append(logits_o[i])
                all_phrases.append(phrases_o[i])
                all_is_hand.append(False)

        if verbose:
            print(f"  Total detections before NMS: {len(all_boxes)}")

        # Apply NMS
        boxes_nms, logits_nms, phrases_nms, is_hand_nms = nms_detections(
            all_boxes, all_logits, all_phrases, all_is_hand, iou_thresh=NMS_THRESHOLD
        )

        if verbose:
            print(f"  After NMS: {len(boxes_nms)}")

        if len(boxes_nms) == 0:
            return (None, []) if return_detections else None

        # Generate SAM2 masks
        sam_predictor.set_image(image_source)

        detections = []
        for i, (box, logit, phrase, is_hand) in enumerate(zip(boxes_nms, logits_nms, phrases_nms, is_hand_nms)):
            box_np = box.cpu().numpy() if isinstance(box, torch.Tensor) else box

            # Convert cxcywh -> xyxy
            x0 = (box_np[0] - box_np[2]/2) * W
            y0 = (box_np[1] - box_np[3]/2) * H
            x1 = (box_np[0] + box_np[2]/2) * W
            y1 = (box_np[1] + box_np[3]/2) * H
            box_xyxy = np.array([max(0, x0), max(0, y0), min(W, x1), min(H, y1)])

            mask_output, _, _ = sam_predictor.predict(box=box_xyxy, multimask_output=False)
            mask = process_sam2_mask(mask_output[0])

            is_excluded = any(excl in phrase.lower() for excl in EXCLUDED_LABELS)

            detections.append({
                "id": i,
                "label": phrase.lower(),
                "bbox": box_xyxy,
                "mask": mask,
                "score": logit.item() if isinstance(logit, torch.Tensor) else logit,
                "is_hand": is_hand,
                "is_excluded": is_excluded
            })

        # Build Node Features (9-dim)
        node_features = []
        node_labels = []

        for d in detections:
            x0, y0, x1, y1 = d["bbox"]
            w, h = x1 - x0, y1 - y0
            cx, cy = x0 + w/2, y0 + h/2

            geom = [norm(cx, W), norm(cy, H), norm(w, W), norm(h, H),
                    float(d["mask"].sum()) / float(W * H)]

            is_hand_flag = 1.0 if d["is_hand"] else 0.0
            lbl = d["label"]
            left_flag = 1.0 if ("left" in lbl and "hand" in lbl) else 0.0
            right_flag = 1.0 if ("right" in lbl and "hand" in lbl) else 0.0
            label_id = float(abs(hash(lbl)) % 10_000) / 10_000.0

            node_features.append(geom + [is_hand_flag, left_flag, right_flag, label_id])
            node_labels.append(lbl)

        x = torch.tensor(node_features, dtype=torch.float32)

        # Build Edges (hand -> manipulable object) - FIXED VERSION
        hand_ids = [d["id"] for d in detections if d["is_hand"]]
        manipulable_ids = [d["id"] for d in detections if not d["is_hand"] and not d["is_excluded"]]

        edges = []
        edge_attrs = []

        if verbose:
            print(f"  Checking interactions: {len(hand_ids)} hands √ó {len(manipulable_ids)} objects")

        for hi in hand_ids:
            for oi in manipulable_ids:
                is_interacting, score, details = check_interaction(detections[hi], detections[oi])

                if is_interacting:
                    edges.append([hi, oi])
                    edge_attrs.append([score])

                    if verbose:
                        print(f"    ‚úì {detections[hi]['label']} ‚Üí {detections[oi]['label']}: "
                              f"score={score:.3f} (mask={details['mask_iou']:.3f}, "
                              f"box={details['box_iou']:.3f}, adj={details['adjacency']:.3f})")

        if len(edges) == 0:
            edge_index = torch.empty((2, 0), dtype=torch.long)
            edge_attr = torch.empty((0, 1), dtype=torch.float32)
        else:
            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
            edge_attr = torch.tensor(edge_attrs, dtype=torch.float32)

        if verbose:
            print(f"  Final edges: {len(edges)}")

        # Create PyG Data object
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(detections))
        data.node_labels = node_labels
        data.image_path = str(image_path)
        data.image_size = (H, W)

        return (data, detections) if return_detections else data

    except Exception as e:
        print(f"[ERROR] Frame failed: {image_path} - {e}")
        import traceback
        traceback.print_exc()
        return (None, []) if return_detections else None

# ============================================================
# Visualization Functions
# ============================================================
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import networkx as nx
import numpy as np

def visualize_frame_analysis(image_path, data, detections, save_path=None, figsize=(24, 16)):
    '''
    Create 6-panel visualization:
    1. Original image
    2. Bounding box detections
    3. Segmentation masks
    4. Mask overlay
    5. Interaction graph on image
    6. NetworkX graph diagram
    '''
    image_source, _ = load_image(str(image_path))
    H, W, _ = image_source.shape

    fig = plt.figure(figsize=figsize)
    gs = fig.add_gridspec(2, 3, hspace=0.25, wspace=0.15)

    HAND_COLOR = '#00FF00'
    OBJECT_COLOR = '#00BFFF'
    EXCLUDED_COLOR = '#808080'
    EDGE_COLOR = '#FF4444'

    # Panel 1: Original Image
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(image_source)
    ax1.set_title("Original Image", fontsize=14, fontweight='bold')
    ax1.axis('off')

    # Panel 2: Bounding Boxes
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.imshow(image_source)

    for det in detections:
        x0, y0, x1, y1 = det["bbox"]
        w, h = x1 - x0, y1 - y0

        if det["is_hand"]:
            color, lw = HAND_COLOR, 3
        elif det["is_excluded"]:
            color, lw = EXCLUDED_COLOR, 2
        else:
            color, lw = OBJECT_COLOR, 3

        rect = patches.Rectangle((x0, y0), w, h, linewidth=lw, edgecolor=color, facecolor='none')
        ax2.add_patch(rect)
        ax2.text(x0, y0-5, f"{det['label'][:12]} ({det['score']:.2f})", fontsize=8,
                bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.8),
                color='black', fontweight='bold')

    hand_count = sum(1 for d in detections if d["is_hand"])
    obj_count = sum(1 for d in detections if not d["is_hand"] and not d["is_excluded"])
    ax2.set_title(f"Bounding Boxes\n({hand_count} hands, {obj_count} objects)", fontsize=14, fontweight='bold')
    ax2.axis('off')

    # Panel 3: Segmentation Masks
    ax3 = fig.add_subplot(gs[0, 2])
    mask_viz = np.zeros((H, W, 3), dtype=np.float32)
    np.random.seed(42)

    for det in detections:
        if det["is_hand"]:
            color = np.array([0.2, 1.0, 0.2])
        elif det["is_excluded"]:
            color = np.array([0.5, 0.5, 0.5])
        else:
            color = np.random.rand(3) * 0.5 + 0.5

        for c in range(3):
            mask_viz[:, :, c][det["mask"]] = color[c]

    ax3.imshow(mask_viz)
    ax3.set_title("Segmentation Masks\n(Green=Hands)", fontsize=14, fontweight='bold')
    ax3.axis('off')

    # Panel 4: Mask Overlay
    ax4 = fig.add_subplot(gs[1, 0])
    overlay = image_source.copy().astype(np.float32)

    for det in detections:
        if det["is_hand"]:
            color = np.array([50, 255, 50])
        elif det["is_excluded"]:
            color = np.array([128, 128, 128])
        else:
            np.random.seed(det["id"] + 100)
            color = np.random.randint(100, 255, 3).astype(np.float32)

        for c in range(3):
            overlay[:, :, c][det["mask"]] = overlay[:, :, c][det["mask"]] * 0.5 + color[c] * 0.5

    ax4.imshow(np.clip(overlay, 0, 255).astype(np.uint8))
    ax4.set_title("Mask Overlay", fontsize=14, fontweight='bold')
    ax4.axis('off')

    # Panel 5: Interaction Graph on Image
    ax5 = fig.add_subplot(gs[1, 1])
    ax5.imshow(image_source)

    if data is not None and data.edge_index.numel() > 0:
        edge_index = data.edge_index.numpy()
        edge_attr = data.edge_attr.numpy()
        interacting_ids = set(edge_index.flatten())
    else:
        edge_index = np.array([[], []])
        edge_attr = np.array([])
        interacting_ids = set()

    for det in detections:
        if det["id"] in interacting_ids or det["is_hand"]:
            x0, y0, x1, y1 = det["bbox"]
            color = HAND_COLOR if det["is_hand"] else '#FFA500'
            rect = patches.Rectangle((x0, y0), x1-x0, y1-y0, linewidth=4, edgecolor=color, facecolor='none')
            ax5.add_patch(rect)
            ax5.text(x0, y0-8, det["label"][:10], fontsize=10,
                    bbox=dict(facecolor=color, alpha=0.9), color='black', fontweight='bold')

    for i in range(edge_index.shape[1]):
        src_bbox = detections[edge_index[0, i]]["bbox"]
        tgt_bbox = detections[edge_index[1, i]]["bbox"]
        src_center = [(src_bbox[0] + src_bbox[2])/2, (src_bbox[1] + src_bbox[3])/2]
        tgt_center = [(tgt_bbox[0] + tgt_bbox[2])/2, (tgt_bbox[1] + tgt_bbox[3])/2]

        ax5.annotate('', xy=tgt_center, xytext=src_center,
                    arrowprops=dict(arrowstyle='->', color=EDGE_COLOR, lw=3, alpha=0.8))

        mid = [(src_center[0]+tgt_center[0])/2, (src_center[1]+tgt_center[1])/2]
        iou = edge_attr[i, 0] if len(edge_attr) > 0 else 0
        ax5.text(mid[0], mid[1], f"{iou:.2f}", fontsize=10, color='white',
                bbox=dict(facecolor=EDGE_COLOR, alpha=0.9), ha='center', fontweight='bold')

    if edge_index.shape[1] == 0:
        ax5.text(W/2, H/2, "No interactions", fontsize=16, color='red', ha='center', va='center',
                bbox=dict(facecolor='white', alpha=0.8))

    ax5.set_title(f"Interactions ({edge_index.shape[1]} edges)", fontsize=14, fontweight='bold')
    ax5.axis('off')

    # Panel 6: NetworkX Graph
    ax6 = fig.add_subplot(gs[1, 2])

    if data is not None and (data.edge_index.numel() > 0 or any(d["is_hand"] for d in detections)):
        G = nx.DiGraph()
        nodes_to_add = {d["id"] for d in detections if d["is_hand"] or d["id"] in interacting_ids}

        for det in detections:
            if det["id"] in nodes_to_add:
                G.add_node(det["id"], label=det["label"], is_hand=det["is_hand"])

        for i in range(edge_index.shape[1]):
            iou = float(edge_attr[i, 0]) if len(edge_attr) > 0 else 0
            G.add_edge(int(edge_index[0, i]), int(edge_index[1, i]), weight=iou)

        if len(G.nodes()) > 0:
            pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
            hand_nodes = [n for n in G.nodes() if G.nodes[n].get("is_hand")]
            obj_nodes = [n for n in G.nodes() if not G.nodes[n].get("is_hand")]

            nx.draw_networkx_nodes(G, pos, nodelist=hand_nodes, node_color='lightgreen', node_size=2000, node_shape='o', ax=ax6)
            nx.draw_networkx_nodes(G, pos, nodelist=obj_nodes, node_color='orange', node_size=2000, node_shape='s', ax=ax6)
            nx.draw_networkx_edges(G, pos, edge_color='red', arrows=True, arrowsize=25, width=2, ax=ax6)

            labels = {n: G.nodes[n]["label"][:8] for n in G.nodes()}
            nx.draw_networkx_labels(G, pos, labels, font_size=9, font_weight='bold', ax=ax6)

            edge_labels = {(u, v): f"{d['weight']:.2f}" for u, v, d in G.edges(data=True)}
            nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8, ax=ax6)
    else:
        ax6.text(0.5, 0.5, "No interactions", fontsize=14, ha='center', va='center')

    ax6.set_title("Graph Diagram\n(Circle=Hand, Square=Object)", fontsize=14, fontweight='bold')
    ax6.axis('off')

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
        print(f"Saved: {save_path}")
    plt.show()
    return fig

# ============================================================
# Demo: Process a single frame and visualize
# ============================================================

# Pick a sample PNR frame
if len(extracted_clips) > 0:
    sample_clip = extracted_clips[2]
    pnr_frames_list = [f for f in sample_clip["frames"] if f["is_pnr"]]
    sample_frame_info = pnr_frames_list[0] if pnr_frames_list else sample_clip["frames"][len(sample_clip["frames"])//2]
    SAMPLE_IMAGE = sample_frame_info["path"]
    print(f"Processing: {SAMPLE_IMAGE}")
    print(f"Frame idx: {sample_frame_info['frame_idx']}, Is PNR: {sample_frame_info['is_pnr']}")
else:
    print("No extracted clips found. Run the frame extraction cell first.")
    SAMPLE_IMAGE = None

if SAMPLE_IMAGE:
    print("\nRunning detection pipeline...")
    data, detections = process_frame_to_graph(SAMPLE_IMAGE, return_detections=True, verbose=True)

    if data is not None:
        print(f"\n‚úÖ Graph created!")
        print(f"   Nodes: {data.num_nodes}")
        print(f"   Edges: {data.edge_index.shape[1]}")
        print(f"   Feature dim: {data.x.shape[1]}")

        save_path = "/content/drive/MyDrive/CIS 6800 Final Project/frame_analysis.png"
        visualize_frame_analysis(SAMPLE_IMAGE, data, detections, save_path=save_path)
    else:
        print("\n‚ùå No detections found")

# ============================================================
# Inspect PyTorch Geometric Data Object
# ============================================================

if data is not None:
    print("=" * 60)
    print("PyTorch Geometric Data Object")
    print("=" * 60)

    print(f"\nüìä {data}")
    print(f"\nüî¢ Nodes: {data.num_nodes}")
    print(f"üîó Edges: {data.edge_index.shape[1]}")

    print(f"\nüìê Node Features (x): shape={data.x.shape}")
    print("   [0] cx       - normalized center x")
    print("   [1] cy       - normalized center y")
    print("   [2] w        - normalized width")
    print("   [3] h        - normalized height")
    print("   [4] area     - mask area ratio")
    print("   [5] is_hand  - 1.0 if hand")
    print("   [6] left     - 1.0 if left hand")
    print("   [7] right    - 1.0 if right hand")
    print("   [8] label_id - semantic hash")

    print(f"\nüîó Edge Index: {data.edge_index.shape}")
    if data.edge_index.numel() > 0:
        print(f"   Edges: {data.edge_index.t().tolist()}")

    print(f"\nüìè Edge Attr (Interaction Score): {data.edge_attr.shape}")
    if data.edge_attr.numel() > 0:
        print(f"   Values: {data.edge_attr.squeeze().tolist()}")

    print(f"\nüè∑Ô∏è Labels: {data.node_labels}")

    print("\n" + "=" * 60)
    print(f"ALL Node Features ({data.num_nodes} nodes)")
    print("=" * 60)

    # Show ALL nodes instead of just 3
    for i in range(data.num_nodes):
        f = data.x[i].numpy()
        hand_marker = "üñêÔ∏è" if f[5] == 1.0 else "üì¶"

        print(f"\n{hand_marker} Node {i} ({data.node_labels[i]}):")
        print(f"   Position:  cx={f[0]:.3f}, cy={f[1]:.3f}")
        print(f"   Size:      w={f[2]:.3f}, h={f[3]:.3f}")
        print(f"   Mask area: {f[4]:.4f}")
        print(f"   Hand:      is_hand={f[5]:.0f}, left={f[6]:.0f}, right={f[7]:.0f}")
        print(f"   Label ID:  {f[8]:.4f}")

    # Also show edge details
    if data.edge_index.numel() > 0:
        print("\n" + "=" * 60)
        print(f"ALL Edges ({data.edge_index.shape[1]} interactions)")
        print("=" * 60)

        for i in range(data.edge_index.shape[1]):
            src = data.edge_index[0, i].item()
            tgt = data.edge_index[1, i].item()
            score = data.edge_attr[i].item()

            print(f"\n   Edge {i}: {data.node_labels[src]} ‚Üí {data.node_labels[tgt]}")
            print(f"            Node {src} ‚Üí Node {tgt}")
            print(f"            Score: {score:.4f}")

else:
    print("No data. Run the processing cell first.")

# ============================================================
# CELL 7: Batch Process All Clips -> Save Graph Sequences
# ============================================================
import os
import gc
from tqdm import tqdm
from pathlib import Path
from collections import Counter

OUTPUT_GRAPHS_ROOT = Path("/content/drive/MyDrive/CIS 6800 Final Project/ego4d_data/v2/graphs_v2")
OUTPUT_GRAPHS_ROOT.mkdir(parents=True, exist_ok=True)

# Track statistics
stats = {
    "total_clips": 0,
    "total_frames": 0,
    "frames_with_graphs": 0,
    "frames_with_edges": 0,
    "total_edges": 0,
    "skipped": 0
}

# Process each extracted clip (outer progress bar)
for clip_info in tqdm(extracted_clips, desc="Clips", position=0):
    clip_id = clip_info["clip_id"]
    split = clip_info["split"]

    split_dir = OUTPUT_GRAPHS_ROOT / split
    split_dir.mkdir(exist_ok=True)

    # Skip if already processed (useful for resuming)
    save_path = split_dir / f"{clip_id}.pt"
    if save_path.exists():
        stats["skipped"] += 1
        continue

    clip_graphs = []

    # Inner progress bar for frames within clip
    for frame_info in tqdm(clip_info["frames"],
                           desc=f"  Frames ({clip_id[:8]}...)",
                           position=1,
                           leave=False):
        graph = process_frame_to_graph(frame_info["path"], return_detections=False, verbose=False)

        clip_graphs.append({
            "frame_idx": frame_info["frame_idx"],
            "is_pnr": frame_info["is_pnr"],
            "relative_to_pnr": frame_info["relative_to_pnr"],
            "graph": graph
        })

        # Update stats
        stats["total_frames"] += 1
        if graph is not None:
            stats["frames_with_graphs"] += 1
            if graph.edge_index.shape[1] > 0:
                stats["frames_with_edges"] += 1
                stats["total_edges"] += graph.edge_index.shape[1]

    # Save clip graphs
    torch.save(clip_graphs, save_path)
    stats["total_clips"] += 1

    # Clear GPU memory periodically
    if stats["total_clips"] % 10 == 0:
        torch.cuda.empty_cache()
        gc.collect()

# Print summary
print("\n" + "=" * 60)
print("PROCESSING COMPLETE")
print("=" * 60)
print(f"‚úÖ Clips processed: {stats['total_clips']}")
print(f"‚è≠Ô∏è Clips skipped (already done): {stats['skipped']}")
print(f"‚úÖ Total frames: {stats['total_frames']}")
print(f"‚úÖ Frames with detections: {stats['frames_with_graphs']} ({100*stats['frames_with_graphs']/(stats['total_frames']+1e-6):.1f}%)")
print(f"‚úÖ Frames with interactions: {stats['frames_with_edges']} ({100*stats['frames_with_edges']/(stats['total_frames']+1e-6):.1f}%)")
print(f"‚úÖ Total interaction edges: {stats['total_edges']}")
print(f"‚úÖ Avg edges per frame: {stats['total_edges']/(stats['frames_with_edges']+1e-6):.2f}")

print(f"\nüìÅ Saved to: {OUTPUT_GRAPHS_ROOT}")

# Count files per split
for split in ["train", "val", "test"]:
    split_path = OUTPUT_GRAPHS_ROOT / split
    if split_path.exists():
        count = len(list(split_path.glob("*.pt")))
        print(f"   {split}: {count} clips")

"""# Training the Graph Encoder"""

# ============================================================
# CELL 11: Clip-Level Dataset for Temporal Modeling (TRAIN-TIME RANDOM PNR)
# ‚úÖ FIXED: No more <17 hard filter, no empty-graph rejection
# ============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GCNConv, GATConv, GraphNorm
from torch_geometric.nn import global_mean_pool, global_max_pool
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from tqdm import tqdm
import numpy as np
import random

# -----------------------------
# CONFIG
# -----------------------------
GRAPHS_ROOT = Path("/content/drive/MyDrive/CIS 6800 Final Project/ego4d_data/v2/graphs")

CENTER_PNR = 8     # because disk clips originally have 17 frames
MIN_WINDOW = 3
MAX_WINDOW = 10

# -----------------------------
# DATASET
# -----------------------------

class PNRClipDataset(Dataset):
    """
    Dataset that returns FULL CLIPS (sequences of graphs).
    Random pruning can be ENABLED or DISABLED per split.
    """

    def __init__(self, root, split='train', random_prune=False):
        self.split = split
        self.random_prune = random_prune   # ‚úÖ explicit control
        self.graph_dir = Path(root) / split
        self.clips = []

        clip_files = sorted(list(self.graph_dir.glob("*.pt")))
        print(f"Loading {split} clips from {len(clip_files)} files...")

        for clip_file in tqdm(clip_files, desc=f"Loading {split}"):
            clip_data = torch.load(clip_file, weights_only=False)

            frames = []
            pnr_idx = None

            for i, frame_data in enumerate(clip_data):
                graph = frame_data["graph"]

                # ‚úÖ KEEP EMPTY GRAPHS ‚Äî DO NOT DROP FRAMES ANYMORE
                if graph is not None:
                    frames.append({
                        "graph": graph,
                        "is_pnr": frame_data["is_pnr"],
                        "relative_to_pnr": frame_data["relative_to_pnr"]
                    })

                    if frame_data["is_pnr"]:
                        pnr_idx = len(frames) - 1

            # ‚úÖ ONLY REQUIRE:
            #   1. PNR must exist
            #   2. At least MIN_WINDOW frames must survive
            if pnr_idx is not None and len(frames) >= MIN_WINDOW:
                self.clips.append({
                    "frames": frames,
                    "pnr_idx": pnr_idx
                })

        print(f"‚úÖ Loaded {len(self.clips)} valid {split} clips")

        if len(self.clips) > 0:
            seq_lens = [len(c["frames"]) for c in self.clips]
            print(
                f"   Seq lengths on disk: "
                f"min={min(seq_lens)}, max={max(seq_lens)}, avg={np.mean(seq_lens):.1f}"
            )

    def __len__(self):
        return len(self.clips)

    def __getitem__(self, idx):
        clip = self.clips[idx]
        frames = clip["frames"]
        pnr_idx = clip["pnr_idx"]
        T = len(frames)

        # ====================================================
        # ‚úÖ RANDOM PRUNING IF ENABLED
        # ====================================================
        if self.random_prune:
            frames_before = random.randint(MIN_WINDOW, MAX_WINDOW)
            frames_after  = random.randint(MIN_WINDOW, MAX_WINDOW)

            start = max(0, pnr_idx - frames_before)
            end   = min(T - 1, pnr_idx + frames_after)

            frames = frames[start:end+1]
            pnr_idx = pnr_idx - start

        # ====================================================
        # REBUILD DISTANCES AFTER PRUNING
        # ====================================================
        graphs = [f["graph"] for f in frames]

        distances = torch.tensor(
            [abs(i - pnr_idx) for i in range(len(frames))],
            dtype=torch.float32
        )

        return {
            "graphs": graphs,
            "pnr_idx": pnr_idx,
            "distances": distances,
            "seq_len": len(graphs)
        }


# -----------------------------
# COLLATE FUNCTION (UNCHANGED)
# -----------------------------

def collate_clips(batch):
    """Custom collate for variable-length clips."""
    return {
        "graphs": [item["graphs"] for item in batch],
        "pnr_idx": torch.tensor([item["pnr_idx"] for item in batch]),
        "distances": [item["distances"] for item in batch],
        "seq_len": torch.tensor([item["seq_len"] for item in batch])
    }


# -----------------------------
# LOAD DATASETS
# -----------------------------

print("=" * 60)
print("LOADING CLIP DATASETS (WITH TRAIN-TIME RANDOMIZATION)")
print("=" * 60)

train_dataset = PNRClipDataset(
    GRAPHS_ROOT, split='train',
    random_prune=True
)

val_dataset = PNRClipDataset(
    GRAPHS_ROOT, split='val',
    random_prune=True
)

test_dataset = PNRClipDataset(
    GRAPHS_ROOT, split='test',
    random_prune=True
)

print(f"\nüìä Dataset Summary:")
print(f"   Train: {len(train_dataset)} clips")
print(f"   Val:   {len(val_dataset)} clips")
print(f"   Test:  {len(test_dataset)} clips")

# ============================================================
# CELL 12: Create DataLoaders
# ============================================================

BATCH_SIZE = 4  # Small batch for clip-level training

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_clips
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_clips
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_clips
)

print(f"‚úÖ DataLoaders created:")
print(f"   Train: {len(train_loader)} batches")
print(f"   Val:   {len(val_loader)} batches")
print(f"   Test:  {len(test_loader)} batches")

# -----------------------------
# QUICK TRAIN-TIME RANDOMIZATION TEST
# -----------------------------

batch = next(iter(train_loader))

print(f"\nüì¶ Sample TRAIN batch:")
print(f"   Clips in batch: {len(batch['graphs'])}")
print(f"   Seq lengths (RANDOMIZED): {batch['seq_len'].tolist()}")
print(f"   PNR indices (RANDOMIZED): {batch['pnr_idx'].tolist()}")

# ============================================================
# CELL 13: Verify PNR Distribution (Train + Val + Test)
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

N_BATCHES_TO_SAMPLE = 100


def collect_stats(loader, name):
    pnr_positions = []
    seq_lens = []

    print(f"\nüîç Sampling {name}_loader to verify PNR distribution...")

    for i, batch in enumerate(tqdm(loader)):
        if i >= N_BATCHES_TO_SAMPLE:
            break

        pnr_idx = batch["pnr_idx"]     # [B]
        seq_len = batch["seq_len"]     # [B]

        B = len(seq_len)

        for b in range(B):
            T = int(seq_len[b].item())
            seq_lens.append(T)

            if T > 1:
                rel_pos = pnr_idx[b].item() / (T - 1)
            else:
                rel_pos = 0.5

            pnr_positions.append(rel_pos)

    return np.array(pnr_positions), np.array(seq_lens)


# ============================================================
# COLLECT STATS
# ============================================================

train_pos, train_len = collect_stats(train_loader, "TRAIN")
val_pos,   val_len   = collect_stats(val_loader,   "VAL")
test_pos,  test_len  = collect_stats(test_loader,  "TEST")


# ============================================================
# PNR POSITION HISTOGRAMS
# ============================================================

plt.figure(figsize=(14, 4))

plt.subplot(1, 3, 1)
plt.hist(train_pos, bins=20, edgecolor="black", alpha=0.7)
plt.axvline(0.5, color="red", linestyle="--")
plt.title("Train PNR Distribution (Random)")
plt.xlabel("Relative PNR")
plt.ylabel("Count")

plt.subplot(1, 3, 2)
plt.hist(val_pos, bins=20, edgecolor="black", alpha=0.7)
plt.axvline(0.5, color="red", linestyle="--")
plt.title("Val PNR Distribution (Should Match Train)")
plt.xlabel("Relative PNR")

plt.subplot(1, 3, 3)
plt.hist(test_pos, bins=20, edgecolor="black", alpha=0.7)
plt.axvline(0.5, color="red", linestyle="--")
plt.title("Test PNR Distribution (Should Be Centered)")
plt.xlabel("Relative PNR")

plt.tight_layout()
plt.show()


# ============================================================
# SEQUENCE LENGTH HISTOGRAMS
# ============================================================

plt.figure(figsize=(14, 4))

plt.subplot(1, 3, 1)
plt.hist(train_len, bins=range(train_len.min(), train_len.max() + 2), edgecolor="black")
plt.title("Train Seq Lengths")

plt.subplot(1, 3, 2)
plt.hist(val_len, bins=range(val_len.min(), val_len.max() + 2), edgecolor="black")
plt.title("Val Seq Lengths")

plt.subplot(1, 3, 3)
plt.hist(test_len, bins=range(test_len.min(), test_len.max() + 2), edgecolor="black")
plt.title("Test Seq Lengths")

plt.tight_layout()
plt.show()


# ============================================================
# NUMERICAL STATS
# ============================================================

def print_stats(name, pos, length):
    print(f"\nüìä {name} STATS")
    print(f"   Mean PNR Position: {pos.mean():.3f}")
    print(f"   Std  PNR Position: {pos.std():.3f}")
    print(f"   Min  PNR Position: {pos.min():.3f}")
    print(f"   Max  PNR Position: {pos.max():.3f}")
    print(f"   Seq Len Min: {length.min()}")
    print(f"   Seq Len Max: {length.max()}")
    print(f"   Seq Len Avg: {length.mean():.2f}")


print_stats("TRAIN", train_pos, train_len)
print_stats("VAL",   val_pos,   val_len)
print_stats("TEST",  test_pos,  test_len)


# ============================================================
# BASELINE CHECK (TRAIN + VAL)
# ============================================================

def baseline_middle_acc(loader, name):
    correct = 0
    total = 0

    for i, batch in enumerate(loader):
        if i >= N_BATCHES_TO_SAMPLE:
            break

        pnr_idx = batch["pnr_idx"]
        seq_len = batch["seq_len"]

        for b in range(len(seq_len)):
            T = int(seq_len[b].item())
            pred_middle = T // 2

            if pred_middle == pnr_idx[b].item():
                correct += 1
            total += 1

    acc = correct / total
    print(f"   {name} Baseline (Always Middle): {acc:.3f} ({correct}/{total})")


print("\n" + "=" * 60)
print("BASELINE CHECK: Always Predict Middle")
print("=" * 60)
baseline_middle_acc(train_loader, "TRAIN")
baseline_middle_acc(val_loader,   "VAL")

# ============================================================
# CELL X: ORIGINAL PNR TEMPORAL GRAPH MODEL (BASELINE)
# ============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Batch
from torch_geometric.nn import GCNConv, GraphNorm
from torch_geometric.nn import global_mean_pool, global_max_pool


# ============================================================
# ORIGINAL Per-Frame Graph Encoder (BASELINE)
# ============================================================

class PerFrameGNN(nn.Module):
    """Per-frame graph encoder: G_t -> z_t (BASELINE)"""

    def __init__(
        self,
        in_channels=9,
        hidden_channels=32,    # ‚úÖ ORIGINAL
        out_channels=64,      # ‚úÖ ORIGINAL
        num_layers=2,         # ‚úÖ ORIGINAL
        dropout=0.25
    ):
        super().__init__()

        self.input_proj = nn.Linear(in_channels, hidden_channels)

        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        for _ in range(num_layers):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
            self.norms.append(GraphNorm(hidden_channels))

        self.dropout = dropout

        # mean + max pooling ‚Üí 2 * hidden
        self.output_proj = nn.Linear(hidden_channels * 2, out_channels)

    def forward(self, x, edge_index, batch):
        """
        x:           [num_nodes, 9]
        edge_index:  [2, num_edges]
        batch:       [num_nodes]
        """
        x = F.relu(self.input_proj(x))   # [N, 32]

        for conv, norm in zip(self.convs, self.norms):
            x_res = x
            x = conv(x, edge_index)     # message passing
            x = norm(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = x + x_res               # residual

        z_mean = global_mean_pool(x, batch)   # [B, 32]
        z_max  = global_max_pool(x, batch)    # [B, 32]
        z = torch.cat([z_mean, z_max], dim=1) # [B, 64]

        return self.output_proj(z)            # [B, 64]


# ============================================================
# ORIGINAL Temporal Transformer (BASELINE)
# ============================================================

class TemporalTransformer(nn.Module):
    """Temporal encoder: (z_1, ..., z_T) -> (zÃÉ_1, ..., zÃÉ_T)"""

    def __init__(
        self,
        embed_dim=64,        # ‚úÖ ORIGINAL
        num_heads=4,
        num_layers=2,       # ‚úÖ ORIGINAL
        dropout=0.25,
        max_seq_len=32
    ):
        super().__init__()

        self.pos_embedding = nn.Parameter(
            torch.randn(1, max_seq_len, embed_dim) * 0.02
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=embed_dim * 2,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, z_seq, mask=None):
        """
        z_seq: [batch_size, seq_len, embed_dim]
        mask:  [batch_size, seq_len] (True = padded)
        """
        B, T, D = z_seq.shape
        z_seq = z_seq + self.pos_embedding[:, :T, :]
        z_seq = self.transformer(z_seq, src_key_padding_mask=mask)
        return self.norm(z_seq)


# ============================================================
# ORIGINAL Full PNR Model (BASELINE)
# ============================================================

class PNRTemporalModel(nn.Module):
    """
    Full baseline model:
    Graph sequence ‚Üí Per-frame GNN ‚Üí Temporal Transformer ‚Üí Distance Head
    """

    def __init__(
        self,
        node_feat_dim=9,
        gnn_hidden=32,              # ‚úÖ ORIGINAL
        embed_dim=64,              # ‚úÖ ORIGINAL
        num_gnn_layers=2,          # ‚úÖ ORIGINAL
        num_transformer_layers=2, # ‚úÖ ORIGINAL
        num_heads=4,
        dropout=0.25,
        max_seq_len=32
    ):
        super().__init__()

        self.gnn = PerFrameGNN(
            in_channels=node_feat_dim,
            hidden_channels=gnn_hidden,
            out_channels=embed_dim,
            num_layers=num_gnn_layers,
            dropout=dropout
        )

        self.temporal = TemporalTransformer(
            embed_dim=embed_dim,
            num_heads=num_heads,
            num_layers=num_transformer_layers,
            dropout=dropout,
            max_seq_len=max_seq_len
        )

        # ‚úÖ Distance regression head (used for PNR classification via argmax)
        self.distance_head = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, 1)
        )

        self.embed_dim = embed_dim

    def encode_graphs(self, graphs_list):
        """Encode a list of graph sequences into embeddings."""
        batch_embeddings = []

        for graphs in graphs_list:
            batched = Batch.from_data_list(graphs)
            z = self.gnn(batched.x, batched.edge_index, batched.batch)
            batch_embeddings.append(z)

        return batch_embeddings

    def forward(self, graphs_list, seq_lens):
        """
        Returns:
            z_tilde: [batch_size, max_seq_len, embed_dim]
            dist_pred: [batch_size, max_seq_len]
            mask: [batch_size, max_seq_len]
        """
        device = next(self.parameters()).device
        batch_size = len(graphs_list)
        max_len = max(seq_lens).item()

        z_list = self.encode_graphs(graphs_list)

        z_padded = torch.zeros(batch_size, max_len, self.embed_dim, device=device)
        mask = torch.ones(batch_size, max_len, dtype=torch.bool, device=device)

        for i, (z, length) in enumerate(zip(z_list, seq_lens)):
            z_padded[i, :length] = z
            mask[i, :length] = False

        z_tilde = self.temporal(z_padded, mask=mask)
        dist_pred = self.distance_head(z_tilde).squeeze(-1)

        return z_tilde, dist_pred, mask


# ============================================================
# INITIALIZE ORIGINAL BASELINE MODEL
# ============================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = PNRTemporalModel(
    node_feat_dim=9,
    gnn_hidden=32,
    embed_dim=64,
    num_gnn_layers=2,
    num_transformer_layers=2,
    num_heads=4,
    dropout=0.25,
    max_seq_len=32
).to(device)

print(f"‚úÖ ORIGINAL BASELINE model created on {device}")
print(model)

total_params = sum(p.numel() for p in model.parameters())
print(f"\nüìä Total parameters (BASELINE): {total_params:,}")

# ============================================================
# CELL 14: Loss Functions (FIXED: Classification + Distance + Contrastive)
# ============================================================

class PNRContrastiveLoss(nn.Module):
    """
    Weak regularizer: cluster near-PNR embeddings, separate far ones.
    """
    def __init__(self, temperature=0.2, near_threshold=2, far_threshold=6):
        super().__init__()
        self.temperature = temperature
        self.near_threshold = near_threshold
        self.far_threshold = far_threshold

    def forward(self, z_tilde, distances, mask):
        batch_size = z_tilde.shape[0]
        total_loss = 0.0
        valid_clips = 0

        for b in range(batch_size):
            valid_mask = ~mask[b]
            z_valid = z_tilde[b][valid_mask]
            dist_valid = distances[b].to(z_valid.device)

            T = z_valid.shape[0]
            if T < 4:
                continue

            z_norm = F.normalize(z_valid, dim=1)
            near_mask = dist_valid <= self.near_threshold
            far_mask  = dist_valid >= self.far_threshold

            near_idx = torch.where(near_mask)[0]
            far_idx  = torch.where(far_mask)[0]

            if len(near_idx) < 2 or len(far_idx) < 1:
                continue

            clip_loss = 0.0
            for anchor_idx in near_idx:
                anchor = z_norm[anchor_idx]
                pos_idx = near_idx[near_idx != anchor_idx]

                pos_sim = torch.matmul(z_norm[pos_idx], anchor) / self.temperature
                neg_sim = torch.matmul(z_norm[far_idx], anchor) / self.temperature

                clip_loss += -torch.log(
                    torch.exp(pos_sim).mean() /
                    (torch.exp(pos_sim).mean() + torch.exp(neg_sim).mean() + 1e-8)
                )

            total_loss += clip_loss / len(near_idx)
            valid_clips += 1

        if valid_clips == 0:
            return torch.tensor(0.0, device=z_tilde.device)

        return total_loss / valid_clips


class DistanceRegressionLoss(nn.Module):
    """
    Auxiliary smooth distance supervision.
    """
    def __init__(self, max_distance=16):
        super().__init__()
        self.max_distance = max_distance

    def forward(self, dist_pred, distances, mask):
        total_loss = 0.0
        count = 0

        for b in range(dist_pred.shape[0]):
            valid_mask = ~mask[b]
            pred = dist_pred[b][valid_mask]
            target = distances[b].to(pred.device)

            target_norm = target / self.max_distance
            pred_norm = torch.sigmoid(pred)

            total_loss += F.mse_loss(pred_norm, target_norm)
            count += 1

        return total_loss / max(count, 1)


class PNRClassificationLoss(nn.Module):
    """
    ‚úÖ MAIN TASK: Pick the exact PNR index (cross-entropy)
    """
    def forward(self, dist_pred, pnr_indices, mask):
        logits = dist_pred.clone()
        logits[mask] = -1e9  # mask padded frames
        return F.cross_entropy(logits, pnr_indices)


# ‚úÖ Initialize losses
contrastive_loss_fn = PNRContrastiveLoss()
distance_loss_fn = DistanceRegressionLoss()
pnr_ce_loss_fn = PNRClassificationLoss()

# ‚úÖ Balanced weights (CRITICAL)
LAMBDA_PNR = 1.0           # main task
LAMBDA_DISTANCE = 0.1     # auxiliary
LAMBDA_CONTRASTIVE = 0.05 # weak regularizer

print("‚úÖ FIXED multi-task loss functions defined")

# Commented out IPython magic to ensure Python compatibility.
# ============================================================
# CELL 16.5: Launch TensorBoard in Colab
# ============================================================

# Load TensorBoard extension
# %load_ext tensorboard

# Launch TensorBoard
# %tensorboard --logdir "/content/drive/MyDrive/CIS 6800 Final Project/runs"

# ============================================================
# CELL 15: FIXED Training Loop with TensorBoard (PNR SUPERVISED)
# ============================================================

from sklearn.metrics import accuracy_score
from torch.utils.tensorboard import SummaryWriter
import time
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# ------------------------------------------------------------
# TensorBoard setup
# ------------------------------------------------------------

LOG_DIR = "/content/drive/MyDrive/CIS 6800 Final Project/runs"
run_name = f"pnr_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
writer = SummaryWriter(log_dir=f"{LOG_DIR}/{run_name}")

print(f"üìä TensorBoard logs: {LOG_DIR}/{run_name}")

# ------------------------------------------------------------
# Hyperparameters
# ------------------------------------------------------------

NUM_EPOCHS = 100
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 1e-4

# ‚úÖ Multi-task loss weights (IMPORTANT)
LAMBDA_PNR = 1.0
LAMBDA_DISTANCE = 0.1
LAMBDA_CONTRASTIVE = 0.05

# Log hyperparameters
writer.add_hparams(
    {
        'lr': LEARNING_RATE,
        'weight_decay': WEIGHT_DECAY,
        'lambda_pnr': LAMBDA_PNR,
        'lambda_distance': LAMBDA_DISTANCE,
        'lambda_contrastive': LAMBDA_CONTRASTIVE,
        'batch_size': BATCH_SIZE,
    },
    {'placeholder': 0}
)

# ------------------------------------------------------------
# Optimizer & Scheduler
# ------------------------------------------------------------

optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=NUM_EPOCHS, eta_min=1e-5
)

# ------------------------------------------------------------
# Training History (EXPANDED)
# ------------------------------------------------------------

history = {
    # Training losses
    'train_loss': [],
    'train_ce': [],
    'train_contrastive': [],
    'train_distance': [],

    # Validation losses (total + components)
    'val_loss': [],
    'val_ce': [],
    'val_contrastive': [],
    'val_distance': [],

    # Validation accuracy metrics
    'val_pnr_acc': [],        # Exact accuracy
    'val_pnr_acc_1': [],      # ¬±1 frame accuracy

    # Validation error metrics
    'val_mean_error': [],
    'val_median_error': [],

    # Learning rate
    'learning_rate': []
}

# ============================================================
# TRAIN ONE EPOCH (FIXED)
# ============================================================

def train_one_epoch(model, loader, optimizer, epoch, writer):
    model.train()

    total_loss = 0
    total_ce = 0
    total_cont = 0
    total_dist = 0

    for batch_idx, batch in enumerate(loader):
        optimizer.zero_grad()

        graphs_list = batch['graphs']
        seq_lens = batch['seq_len']
        distances = batch['distances']
        pnr_indices = batch['pnr_idx'].to(device)

        # Move graphs to device
        for i in range(len(graphs_list)):
            graphs_list[i] = [g.to(device) for g in graphs_list[i]]

        # Forward pass
        z_tilde, dist_pred, mask = model(graphs_list, seq_lens)

        # ‚úÖ FIXED LOSS COMPUTATION
        loss_ce   = pnr_ce_loss_fn(dist_pred, pnr_indices, mask)
        loss_dist = distance_loss_fn(dist_pred, distances, mask)
        loss_cont = contrastive_loss_fn(z_tilde, distances, mask)

        loss = (
            LAMBDA_PNR * loss_ce +
            LAMBDA_DISTANCE * loss_dist +
            LAMBDA_CONTRASTIVE * loss_cont
        )

        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        total_ce += loss_ce.item()
        total_cont += loss_cont.item()
        total_dist += loss_dist.item()

        # Log batch-level metrics
        global_step = epoch * len(loader) + batch_idx
        if batch_idx % 10 == 0:
            writer.add_scalar('Batch/loss', loss.item(), global_step)

    n = len(loader)
    return (
        total_loss / n,
        total_ce / n,
        total_cont / n,
        total_dist / n
    )

# ============================================================
# EVALUATION (FIXED: Returns component losses)
# ============================================================

@torch.no_grad()
def evaluate(model, loader):
    model.eval()

    total_loss = 0
    total_ce = 0
    total_cont = 0
    total_dist = 0

    all_pnr_preds = []
    all_pnr_targets = []
    all_dist_errors = []

    for batch in loader:
        graphs_list = batch['graphs']
        seq_lens = batch['seq_len']
        distances = batch['distances']
        pnr_indices = batch['pnr_idx'].to(device)

        for i in range(len(graphs_list)):
            graphs_list[i] = [g.to(device) for g in graphs_list[i]]

        z_tilde, dist_pred, mask = model(graphs_list, seq_lens)

        # ‚úÖ SAME FIXED LOSSES
        loss_ce   = pnr_ce_loss_fn(dist_pred, pnr_indices, mask)
        loss_dist = distance_loss_fn(dist_pred, distances, mask)
        loss_cont = contrastive_loss_fn(z_tilde, distances, mask)

        loss = (
            LAMBDA_PNR * loss_ce +
            LAMBDA_DISTANCE * loss_dist +
            LAMBDA_CONTRASTIVE * loss_cont
        )

        total_loss += loss.item()
        total_ce += loss_ce.item()
        total_cont += loss_cont.item()
        total_dist += loss_dist.item()

        # ‚úÖ FIXED PNR PREDICTION: ARGMAX (CLASSIFICATION)
        for b in range(len(graphs_list)):
            valid_len = seq_lens[b].item()
            logits = dist_pred[b, :valid_len]

            pred_pnr = logits.argmax().item()
            true_pnr = pnr_indices[b].item()

            all_pnr_preds.append(pred_pnr)
            all_pnr_targets.append(true_pnr)
            all_dist_errors.append(abs(pred_pnr - true_pnr))

    n = len(loader)

    pnr_acc = sum(p == t for p, t in zip(all_pnr_preds, all_pnr_targets)) / len(all_pnr_preds)
    pnr_acc_1 = sum(e <= 1 for e in all_dist_errors) / len(all_dist_errors)

    return {
        # Total and component losses
        'loss': total_loss / n,
        'ce': total_ce / n,
        'contrastive': total_cont / n,
        'distance': total_dist / n,

        # Accuracy metrics
        'pnr_acc': pnr_acc,
        'pnr_acc_1': pnr_acc_1,

        # Error metrics
        'mean_error': np.mean(all_dist_errors),
        'median_error': np.median(all_dist_errors)
    }

# ============================================================
# TRAINING CONFIG SUMMARY
# ============================================================

print("‚úÖ FIXED Training functions with TensorBoard defined")
print(f"\nüéØ Training config:")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   Learning rate: {LEARNING_RATE}")
print(f"   Lambda PNR: {LAMBDA_PNR}")
print(f"   Lambda distance: {LAMBDA_DISTANCE}")
print(f"   Lambda contrastive: {LAMBDA_CONTRASTIVE}")


# ============================================================
# CELL 16: Run Training with Comprehensive Tracking
# ============================================================

print("=" * 60)
print("TRAINING WITH COMPREHENSIVE TRACKING")
print("=" * 60)

best_val_acc = 0
best_model_state = None

for epoch in range(NUM_EPOCHS):
    start_time = time.time()

    # ==================== TRAIN ====================
    train_loss, train_ce, train_cont, train_dist = train_one_epoch(
        model, train_loader, optimizer, epoch, writer
    )

    # ==================== VALIDATE ====================
    val_metrics = evaluate(model, val_loader)

    # ==================== SCHEDULER ====================
    current_lr = scheduler.get_last_lr()[0]
    scheduler.step()

    # ==================== HISTORY (EXPANDED) ====================

    # Training losses
    history['train_loss'].append(train_loss)
    history['train_ce'].append(train_ce)
    history['train_contrastive'].append(train_cont)
    history['train_distance'].append(train_dist)

    # Validation losses (total + components)
    history['val_loss'].append(val_metrics['loss'])
    history['val_ce'].append(val_metrics['ce'])
    history['val_contrastive'].append(val_metrics['contrastive'])
    history['val_distance'].append(val_metrics['distance'])

    # Validation accuracy
    history['val_pnr_acc'].append(val_metrics['pnr_acc'])
    history['val_pnr_acc_1'].append(val_metrics['pnr_acc_1'])

    # Validation error
    history['val_mean_error'].append(val_metrics['mean_error'])
    history['val_median_error'].append(val_metrics['median_error'])

    # Learning rate
    history['learning_rate'].append(current_lr)

    # ==================== TENSORBOARD ====================

    # Training losses
    writer.add_scalar('Loss/train_total', train_loss, epoch)
    writer.add_scalar('Loss/train_ce', train_ce, epoch)
    writer.add_scalar('Loss/train_contrastive', train_cont, epoch)
    writer.add_scalar('Loss/train_distance', train_dist, epoch)

    # Validation losses
    writer.add_scalar('Loss/val_total', val_metrics['loss'], epoch)
    writer.add_scalar('Loss/val_ce', val_metrics['ce'], epoch)
    writer.add_scalar('Loss/val_contrastive', val_metrics['contrastive'], epoch)
    writer.add_scalar('Loss/val_distance', val_metrics['distance'], epoch)

    # Accuracy metrics
    writer.add_scalar('Accuracy/val_exact', val_metrics['pnr_acc'], epoch)
    writer.add_scalar('Accuracy/val_pm1', val_metrics['pnr_acc_1'], epoch)

    # Error metrics
    writer.add_scalar('Error/val_mean', val_metrics['mean_error'], epoch)
    writer.add_scalar('Error/val_median', val_metrics['median_error'], epoch)

    # Learning rate
    writer.add_scalar('LR/learning_rate', current_lr, epoch)

    # Combined plots
    writer.add_scalars('Loss/comparison', {
        'train': train_loss,
        'val': val_metrics['loss']
    }, epoch)

    # ==================== SAVE BEST MODEL ====================

    if val_metrics['pnr_acc_1'] > best_val_acc:
        best_val_acc = val_metrics['pnr_acc_1']
        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        writer.add_scalar('Best/val_pnr_acc_1', best_val_acc, epoch)

    # ==================== PRINT PROGRESS ====================

    elapsed = time.time() - start_time

    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(
            f"Epoch {epoch+1:3d}/{NUM_EPOCHS} | "
            f"Loss: {train_loss:.4f} (ce:{train_ce:.4f}, cont:{train_cont:.4f}, dist:{train_dist:.4f}) | "
            f"Val Acc: {val_metrics['pnr_acc']:.3f} (¬±1: {val_metrics['pnr_acc_1']:.3f}) | "
            f"Val Err: {val_metrics['mean_error']:.2f} | "
            f"LR: {current_lr:.2e} | "
            f"Time: {elapsed:.1f}s"
        )

# ==================== CLEANUP ====================

writer.close()

print("\n" + "=" * 60)
print("TRAINING COMPLETE")
print("=" * 60)
print(f"Best validation PNR Acc (¬±1): {best_val_acc:.4f}")
print(f"\nüìä TensorBoard logs saved to: {LOG_DIR}/{run_name}")

# ============================================================
# CELL 17: PLOTTING (4 subplots) WITH SMOOTHING
# ============================================================

import matplotlib.pyplot as plt
import numpy as np

# ------------------------------------------------------------
# Smoothing function (Exponential Moving Average)
# ------------------------------------------------------------
def smooth(values, weight=0.5):
    """
    Exponential moving average smoothing.
    weight: smoothing factor (0 = no smoothing, 1 = full smoothing)
    """
    smoothed = []
    last = values[0]
    for v in values:
        smoothed_val = last * weight + (1 - weight) * v
        smoothed.append(smoothed_val)
        last = smoothed_val
    return smoothed

SMOOTHING = 0.7

fig, axes = plt.subplots(1, 4, figsize=(20, 5))

epochs = range(1, len(history['train_loss']) + 1)

# ------------------------------------------------------------
# 1. Total Loss: Training + Validation
# ------------------------------------------------------------
axes[0].plot(epochs, history['train_loss'], color='blue', alpha=0.3, linewidth=1)
axes[0].plot(epochs, smooth(history['train_loss'], SMOOTHING), label='Train', color='blue', linewidth=2)
axes[0].plot(epochs, history['val_loss'], color='orange', alpha=0.3, linewidth=1)
axes[0].plot(epochs, smooth(history['val_loss'], SMOOTHING), label='Validation', color='orange', linewidth=2)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Total Loss (Train vs Val)')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# ------------------------------------------------------------
# 2. Training Loss by Component
# ------------------------------------------------------------
axes[1].plot(epochs, history['train_ce'], color='red', alpha=0.3, linewidth=1)
axes[1].plot(epochs, smooth(history['train_ce'], SMOOTHING), label='CE (PNR)', color='red', linewidth=2)
axes[1].plot(epochs, history['train_contrastive'], color='green', alpha=0.3, linewidth=1)
axes[1].plot(epochs, smooth(history['train_contrastive'], SMOOTHING), label='Contrastive', color='green', linewidth=2)
axes[1].plot(epochs, history['train_distance'], color='purple', alpha=0.3, linewidth=1)
axes[1].plot(epochs, smooth(history['train_distance'], SMOOTHING), label='Distance', color='purple', linewidth=2)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].set_title('Training Loss Components')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# ------------------------------------------------------------
# 3. Exact Validation Accuracy
# ------------------------------------------------------------
axes[2].plot(epochs, history['val_pnr_acc'], color='blue', alpha=0.3, linewidth=1)
axes[2].plot(epochs, smooth(history['val_pnr_acc'], SMOOTHING), label='Exact Acc', color='blue', linewidth=2)
axes[2].axhline(y=max(history['val_pnr_acc']), color='blue', linestyle='--', alpha=0.5,
                label=f'Best: {max(history["val_pnr_acc"]):.3f}')
axes[2].set_xlabel('Epoch')
axes[2].set_ylabel('Accuracy')
axes[2].set_title('Exact Validation Accuracy')
axes[2].set_ylim([0, 1])
axes[2].legend()
axes[2].grid(True, alpha=0.3)

# ------------------------------------------------------------
# 4. ¬±1 Frame Validation Accuracy
# ------------------------------------------------------------
axes[3].plot(epochs, history['val_pnr_acc_1'], color='green', alpha=0.3, linewidth=1)
axes[3].plot(epochs, smooth(history['val_pnr_acc_1'], SMOOTHING), label='¬±1 Frame Acc', color='green', linewidth=2)
axes[3].axhline(y=max(history['val_pnr_acc_1']), color='green', linestyle='--', alpha=0.5,
                label=f'Best: {max(history["val_pnr_acc_1"]):.3f}')
axes[3].set_xlabel('Epoch')
axes[3].set_ylabel('Accuracy')
axes[3].set_title('¬±1 Frame Validation Accuracy')
axes[3].set_ylim([0, 1])
axes[3].legend()
axes[3].grid(True, alpha=0.3)

plt.suptitle(f'Training History (smoothing={SMOOTHING})', fontsize=14, y=1.02)
plt.tight_layout()
plt.savefig(
    '/content/drive/MyDrive/CIS 6800 Final Project/training_history.png',
    dpi=150,
    bbox_inches='tight'
)
plt.show()

# ------------------------------------------------------------
# Print Summary Statistics
# ------------------------------------------------------------
print("\n" + "=" * 60)
print("TRAINING SUMMARY")
print("=" * 60)
print(f"Final Train Loss:     {history['train_loss'][-1]:.4f}")
print(f"Final Val Loss:       {history['val_loss'][-1]:.4f}")
print(f"Best Exact Accuracy:  {max(history['val_pnr_acc']):.4f} (epoch {np.argmax(history['val_pnr_acc'])+1})")
print(f"Best ¬±1 Accuracy:     {max(history['val_pnr_acc_1']):.4f} (epoch {np.argmax(history['val_pnr_acc_1'])+1})")
print("=" * 60)
print("\nüìä Training history saved!")

# ============================================================
# CELL 17: Final Evaluation on Test Set
# ============================================================

# Load best model
model.load_state_dict(best_model_state)

# Evaluate on test set
test_metrics = evaluate(model, test_loader)

print("=" * 60)
print("TEST SET RESULTS")
print("=" * 60)
print(f"PNR Localization Accuracy (exact): {test_metrics['pnr_acc']:.4f}")
print(f"PNR Localization Accuracy (¬±1 frame): {test_metrics['pnr_acc_1']:.4f}")
print(f"Mean Absolute Error: {test_metrics['mean_error']:.2f} frames")
print(f"Median Absolute Error: {test_metrics['median_error']:.2f} frames")

# ============================================================
# SANITY CHECK 2 (FIXED): RANDOM PNR BASELINE
# ============================================================

import random
random.seed(0)

correct_exact = 0
correct_within_1 = 0
errors = []
total = 0

for batch in test_loader:   # ‚úÖ Use loader so RANDOM pruning is applied
    seq_lens = batch["seq_len"]
    pnr_idx = batch["pnr_idx"]

    B = len(seq_lens)

    for b in range(B):
        T = seq_lens[b].item()
        true_pnr = pnr_idx[b].item()

        # ‚úÖ RANDOM PREDICTION (true chance baseline)
        pred_pnr = random.randint(0, T - 1)

        error = abs(pred_pnr - true_pnr)
        errors.append(error)

        if pred_pnr == true_pnr:
            correct_exact += 1
        if error <= 1:
            correct_within_1 += 1

        total += 1

print("=" * 50)
print("BASELINE (RANDOM): Predict Random Frame")
print("=" * 50)
print(f"Exact Accuracy: {correct_exact/total:.3f} ({correct_exact}/{total})")
print(f"¬±1 Accuracy: {correct_within_1/total:.3f} ({correct_within_1}/{total})")
print(f"Mean Error: {np.mean(errors):.2f} frames")

# ============================================================
# SANITY CHECK 4: Visualize predictions on test set
# ============================================================

model.eval()
results = []

with torch.no_grad():
    for i, clip in enumerate(test_dataset.clips[:10]):  # First 10
        graphs = [g.to(device) for g in test_dataset[i]["graphs"]]
        seq_len = len(graphs)

        # Get prediction
        z_tilde, dist_pred, mask = model([graphs], torch.tensor([seq_len]))
        pred_dist = torch.sigmoid(dist_pred[0, :seq_len]).cpu().numpy()
        pred_pnr = pred_dist.argmin()
        true_pnr = clip["pnr_idx"]

        results.append({
            "true_pnr": true_pnr,
            "pred_pnr": pred_pnr,
            "seq_len": seq_len,
            "pred_dist": pred_dist
        })

# Plot
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.flatten()

for i, res in enumerate(results):
    ax = axes[i]
    frames = np.arange(res["seq_len"])
    ax.plot(frames, res["pred_dist"], 'b-', label='Predicted dist')
    ax.axvline(res["true_pnr"], color='green', linestyle='--', label=f'True PNR')
    ax.axvline(res["pred_pnr"], color='red', linestyle=':', label=f'Pred PNR')
    ax.set_xlabel('Frame')
    ax.set_ylabel('Predicted Distance')
    ax.set_title(f'Clip {i+1}')
    ax.legend(fontsize=8)

plt.tight_layout()
plt.show()

# ============================================================
# CELL: BASELINE VS MODEL COMPARISON BAR PLOT
# ============================================================

import matplotlib.pyplot as plt
import numpy as np

# ------------------------------------------------------------
# Results Data
# ------------------------------------------------------------

metrics = ['Exact Accuracy', '¬±1 Frame Accuracy', 'Mean Error (frames)']

baseline_values = [0.100, 0.150, 3.45]
model_values = [0.400, 0.700, 1.25]

# For error, lower is better, so we'll handle display accordingly
x = np.arange(len(metrics))
width = 0.35

# ------------------------------------------------------------
# Create Bar Plot
# ------------------------------------------------------------

fig, ax = plt.subplots(figsize=(10, 6))

bars_baseline = ax.bar(x - width/2, baseline_values, width, label='Baseline (Random)', color='#d62728', alpha=0.8)
bars_model = ax.bar(x + width/2, model_values, width, label='Our Model', color='#2ca02c', alpha=0.8)

# ------------------------------------------------------------
# Formatting
# ------------------------------------------------------------

ax.set_ylabel('Score', fontsize=12)
ax.set_title('Test Set Performance: Baseline vs Our Model', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics, fontsize=11)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

add_labels(bars_baseline)
add_labels(bars_model)

plt.tight_layout()
plt.savefig(
    '/content/drive/MyDrive/CIS 6800 Final Project/baseline_vs_model_comparison.png',
    dpi=150,
    bbox_inches='tight'
)
plt.show()

print("\nüìä Comparison plot saved!")